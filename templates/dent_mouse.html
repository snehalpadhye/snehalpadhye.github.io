<!DOCTYPE html>
<!--
     Code for real time illumination demo
     References: https://threejs.org/examples/
               : https://docs.opencv.org/3.4/d8/d54/tutorial_js_imgproc_camera.html

 -->

<html lang="en">
	<head>
		<title>reflection demo</title>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, user-scalable=no, minimum-scale=1.0, maximum-scale=1.0">
    <link rel="stylesheet" type="text/css" href="/static/css/info.css"/>
    <div id="result"></div>
	</head>
	<body>
      <p class="err" id="errorMessage"></p>

		  <div class="dent" id="container">
          <canvas id="canvasOutput" style="width:50px; height:50px;"></canvas>
          <div>
              <video id="videoInput" class="hidden" style="width:50px; height:50px;">Your browser does not support the video tag.</video>
          </div>
      </div>
		<!-- load javascript libraries and functions we're going to need -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
		<script src="{{url_for('static', filename='./build/three.js')}}"></script>
    <script src="{{url_for('static', filename='./js/WebGL.js')}}"></script>
		<script src="{{url_for('static', filename='./js/controls/OrbitControls.js')}}"></script>
		<script src="{{url_for('static', filename='./js/loaders/RGBELoader.js')}}"></script>
		<script src="{{url_for('static', filename='./js/lights/LightProbeGenerator.js')}}"></script>

    <script src="https://webrtc.github.io/adapter/adapter-5.0.4.js" type="text/javascript"></script>
    <script src="/static/js/utils.js" type="text/javascript"></script>
		<script>

			// make sure webGL is available
    if ( WEBGL.isWebGLAvailable() === false ) {
    document.body.appendChild( WEBGL.getWebGLErrorMessage() );
    }

    // define some global variables
    var container;
    var camera, scene, renderer, control, orbit;
    var previous_count = 0;
    var first_frame = true;
    var values =[];


    // create a container for the graphics content
    container = document.createElement('div');
    document.body.appendChild(container);

    // create a renderer for the graphics content
    renderer = new THREE.WebGLRenderer();
    renderer.setSize(window.innerWidth, window.innerHeight);
    renderer.setPixelRatio(window.devicePixelRatio);
    renderer.toneMapping = THREE.ReinhardToneMapping;
    renderer.gammaInput = true;
    renderer.gammaOutput = true;
    // attach the renderer to the document
    //container.appendChild(renderer.domElement);
    document.body.appendChild(renderer.domElement);

    // adding a camera in the scene
    camera = new THREE.PerspectiveCamera(60, window.innerWidth / window.innerHeight, 0.1, 3000);
    camera.position.set(0, 0, 600);

    // orbit controls
    orbit = new THREE.OrbitControls(camera, renderer.domElement);
    orbit.update();

    // create a scene
    scene = new THREE.Scene();

    // initializing a light source
    var light = new THREE.DirectionalLight(0xFFFFFF, 1);
    light.position.set(0, 0, 0);
    scene.add(light);

    //  adding a light probe in the scene
    var lightProbe = new THREE.LightProbe();
    scene.add(lightProbe);

    // loading previously captured images as environment maps
    new THREE.CubeTextureLoader().setPath('static/textures/room/').load(['px.jpg',
                                'nx.jpg',
                                'py.jpg',
                                'ny.jpg',
                                'pz.jpg',
                                'nz.jpg',

        ],
    function (cubeTexture){
        cubeTexture.encoding = THREE.sRGBEncoding;
        scene.background = cubeTexture;
        lightProbe.copy(new THREE.LightProbeGenerator.fromCubeTexture(cubeTexture));

    // creating a plane geometry and aligning it to center
    var geometry = new THREE.PlaneBufferGeometry(512, 312, 32, 8);
    geometry.center(); // plane x,y's start at 0 so center in worldspace

    // creating material for the plane
    var material = new THREE.MeshStandardMaterial({     envMap: cubeTexture,
                                                        map: new THREE.TextureLoader().load('/static/textures/objects/bronze03/diff.png'),
                                                        roughnessMap: new THREE.TextureLoader().load('/static/textures/objects/bronze03/rough.png'),
					                                              normalMap: new THREE.TextureLoader().load('/static/textures/objects/bronze03/norm.png'),
					                                              metalness: 1,
					                                              side: THREE.DoubleSide, envMapIntensity: 1 });

    // creating a mesh for the plane and adding it to the scene
    var objMesh = new THREE.Mesh(geometry, material);
    lightProbe.intensity = 1;
    scene.add(objMesh);

    });

     // image processing part - derived from opencv link in reference
     let utils = new Utils('errorMessage');

     let width = 0;
     let height = 0;

     let resolution = window.innerWidth < 960 ? 'qvga' : 'vga';

     // whether streaming video from the camera.
     let streaming = false;

     let video = document.getElementById('videoInput');
     let vc = null;

     //let container = document.getElementById('container');

     let lastFilter = '';
     let src = null;
     let dstC1 = null;
     let dstC3 = null;
     let dstC4 = null;

     let filters = {
         'gray': 'Gray',
         'threshold': 'Threshold',
     };

     let filterName = document.getElementById('filterName');

     let controls;
     utils.loadOpenCv(() => {
         initUI();
         startCamera();
     });
     function startCamera() {
         if (!streaming) {
             utils.clearError();
             utils.startCamera(resolution, onVideoStarted, 'videoInput');
         } else {
             utils.stopCamera();
             onVideoStopped();
         }
     }
    function initUI() {

        controls = {
            filter: 'passThrough',
            setFilter: function(filter) {
                this.filter = filter;
                filterName.innerHTML = filters[filter];
            },
            gray: function() {
                this.setFilter('gray');
            },
            threshold: function() {
                this.setFilter('threshold');
            },
            thresholdValue: 200,
        };
    }
     function onVideoStarted() {
         height = video.videoHeight;
         width = video.videoWidth;
         video.setAttribute('width', width);
         video.setAttribute('height', height);
         streaming = true;
         vc = new cv.VideoCapture(video);
         startVideoProcessing();
     }

     function startVideoProcessing() {
         src = new cv.Mat(height, width, cv.CV_8UC4);
         dstC1 = new cv.Mat(height, width, cv.CV_8UC1);
         dstC3 = new cv.Mat(height, width, cv.CV_8UC3);
         dstC4 = new cv.Mat(height, width, cv.CV_8UC4);
         requestAnimationFrame(processVideo);
     }

			// scene renderer
			function render(probe_list) {
				  // render the scene
          if (probe_list.length >1 ){
              x = probe_list[1];
              y = probe_list[2];
              z = probe_list[3];
              light.intensity = 1 + (z - 255)/105;
              //console.log('x, y, z - ',x, y, z);
              light.position.set(x, y, z);
          }
				renderer.render(scene, camera);
				}
     function gray(src) {
         cv.cvtColor(src, dstC1, cv.COLOR_RGBA2GRAY);
         return dstC1;
     }
     function threshold(src) {
         controls.thresholdValue = 200;
         cv.threshold(src, dstC4, controls.thresholdValue, 200, cv.THRESH_BINARY);
         return dstC4;
     }

    function processVideo(previous_count) {
        if (!streaming) return;
        vc.read(src);
        let result;
        dst = new cv.Mat();
        result = threshold(src);
        cv.resize(result, dst, new cv.Size(150, 150));
        ge_dst = gray(dst);
        cv.imshow('canvasOutput', ge_dst);
        probe_list = find_light_quadrant(ge_dst, previous_count);
         render(probe_list);
         if (first_frame){first_frame=false;}
         lastFilter = controls.filter;
         if (probe_list.length > 0){
             previous_count = probe_list[0];
         }
        setTimeout(requestAnimationFrame(processVideo, previous_count), 10000);
     }

     // method to find light source and calcuate
     // its coordinates in the scene
     function find_light_quadrant(img, previous_count){
         var xavg = 0;
         var yavg = 0;
         var count = 0;
         var pixel_avg = 0;
         probe_list = [];
         if (!first_frame){
            for (x = 0; x<img.rows; x++){
                for (y = 0; y<img.cols; y++)
                {
                    pixel_arr = img.ucharPtr(x,y);
                    pixel = pixel_arr[0];
                    // second threshold check in image
                    if (pixel > 100){
                        xavg = x + xavg;
                        yavg = y + yavg;
                        pixel_avg = pixel_avg + pixel
                        count = count + 1;
                    }
                }
            }
         // list to refresh light coordinates
         probe_list.push(count);
         if (count > 1000){
             y_new = yavg/count;
             x_new = xavg/count;
             z_new = pixel_avg/count;
             y = 200 + ((400/150)*(x_new - 150))
             x = 200 + ((400/150)*(y_new - 150))
             z = z_new;
             probe_list.push(x);
             probe_list.push(y);
             probe_list.push(z);
         }
         if (count < 100){
             probe_list.push(0);
             probe_list.push(0);
             probe_list.push(0);
         }
         }
         return probe_list
     }


    </script>

	</body>
</html>
